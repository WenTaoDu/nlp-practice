{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"d:\\\\git-nlp\\\\ner-uk\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tokens and positions of tokens from a file\n",
    "\n",
    "def read_tokens(filename):\n",
    "    tokens = []\n",
    "    pos = 0\n",
    "    with open(filename, \"r\", encoding='utf-8') as f:\n",
    "        text = f.read().split(\"\\n\")\n",
    "        for line in text:\n",
    "            if len(line) == 0:\n",
    "                pos += 1\n",
    "            else:\n",
    "                tokens.append((\"<S>\", pos, pos))\n",
    "                for token in line.split(\" \"):\n",
    "                    tokens.append((token, pos, pos + len(token)))\n",
    "                    pos += len(token) + 1\n",
    "                tokens.append((\"</S>\", pos, pos))\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read annotations and positions of annotations from a file\n",
    "\n",
    "def read_annotations(filename):\n",
    "    anno = []\n",
    "    with open(filename, \"r\", encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            annotations = line.split()\n",
    "            #print(annotations)\n",
    "            anno.append((annotations[1], int(annotations[2]), int(annotations[3])))\n",
    "    return anno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using positions of tokens and annotations, extract a list of token labels\n",
    "def cyr_to_lat(label:str):\n",
    "    res = label.replace('ОРГ', 'ORG').replace('ЛОК', 'LOC').replace('ПЕРС', 'PERS').replace('РІЗН', 'MISC')\n",
    "    return res\n",
    "\n",
    "def extract_labels(anno, tokens):\n",
    "    labels = []\n",
    "    ann_id = 0\n",
    "    for token in tokens:\n",
    "        if ann_id < len(anno):\n",
    "            label, beg, end = anno[ann_id]\n",
    "            label = cyr_to_lat(label)\n",
    "            if token[0] in [\"<S>\", \"</S>\"]:\n",
    "                labels.append(\"--\")\n",
    "            elif token[1] < beg:\n",
    "                labels.append(\"--\")\n",
    "            else:\n",
    "                if token[1] == beg:\n",
    "                    labels.append(\"B-\" + label)\n",
    "                else:\n",
    "                    labels.append(\"I-\" + label)\n",
    "                if token[2] == end:\n",
    "                    ann_id += 1\n",
    "        else:\n",
    "            labels.append(\"--\")    \n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = read_tokens(PATH + \"data/A_alumni.krok.edu.ua_Prokopenko_Vidrodzhennia_velotreku(5).tok.txt\")\n",
    "anno = read_annotations(PATH + \"data/A_alumni.krok.edu.ua_Prokopenko_Vidrodzhennia_velotreku(5).tok.ann\")\n",
    "labels = extract_labels(anno, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> --\n",
      "Історія --\n",
      "змін --\n",
      ". --\n",
      "</S> --\n",
      "<S> --\n",
      "Спільними --\n",
      "зусиллями --\n",
      "влада --\n",
      "та --\n",
      "громадськість --\n",
      "врятували --\n",
      "й --\n",
      "повертають --\n",
      "до --\n",
      "життя --\n",
      "Київський B-ORG\n",
      "велотрек I-ORG\n",
      "</S> --\n",
      "<S> --\n",
      "Київський B-ORG\n",
      "велотрек I-ORG\n",
      "« I-ORG\n",
      "Авангард I-ORG\n",
      "» I-ORG\n",
      "по --\n",
      "вул B-LOC\n",
      ". I-LOC\n",
      "Богдана I-LOC\n",
      "Хмельницького I-LOC\n",
      ", I-LOC\n",
      "58-А I-LOC\n",
      ", --\n",
      "що --\n",
      "збудований --\n",
      "у --\n",
      "1913 --\n",
      "році --\n",
      "за --\n",
      "ініціативи --\n",
      "та --\n",
      "кошти --\n",
      "киян --\n",
      ", --\n",
      "відновлюється --\n",
      "так --\n",
      "само --\n",
      "— --\n",
      "силами --\n",
      "громади --\n",
      "і --\n",
      "без --\n",
      "фінансування --\n",
      "з --\n",
      "бюджету --\n",
      ". --\n",
      "</S> --\n",
      "<S> --\n",
      "А --\n",
      "за --\n",
      "відчутної --\n",
      "підтримки --\n",
      "влади --\n",
      "реконструкція --\n",
      "набирає --\n",
      "обертів --\n",
      ". --\n",
      "</S> --\n",
      "<S> --\n",
      "« --\n",
      "Ще --\n",
      "недавно --\n",
      "велотрек --\n",
      "існував --\n",
      "тільки --\n",
      "у --\n",
      "мріях --\n",
      "ентузіастів --\n",
      "велоруху --\n",
      ", --\n",
      "а --\n",
      "вже --\n",
      "зараз --\n",
      "він --\n",
      "стрімко --\n",
      "набирає --\n",
      "реалістичних --\n",
      "контурів --\n",
      ", --\n",
      "— --\n",
      "радіє --\n",
      "голова --\n",
      "Шевченківської B-ORG\n",
      "райдержадміністрації I-ORG\n",
      "Олег B-PERS\n",
      "Гаряга I-PERS\n",
      ". --\n",
      "— --\n",
      "Ми --\n",
      "сподіваємося --\n",
      ", --\n",
      "що --\n",
      "вже --\n",
      "за --\n",
      "півтора-два --\n",
      "місяці --\n",
      "на --\n",
      "велотреку --\n",
      "зможуть --\n",
      "тренуватися --\n",
      "спортсмени --\n",
      ", --\n",
      "а --\n",
      "проводити --\n",
      "змагання --\n",
      "тут --\n",
      "можна --\n",
      "буде --\n",
      "вже --\n",
      "з --\n",
      "наступної --\n",
      "весни --\n",
      ". --\n",
      "</S> --\n",
      "<S> --\n",
      "Роботи --\n",
      "на --\n",
      "самому --\n",
      "велотреку --\n",
      "виконуються --\n",
      "за --\n",
      "кошти --\n",
      "меценатів --\n",
      ", --\n",
      "і --\n",
      "я --\n",
      "вдячний --\n",
      "усім --\n",
      "небайдужим --\n",
      ", --\n",
      "хто --\n",
      "сприяє --\n",
      "розвитку --\n",
      "цього --\n",
      "проекту --\n",
      "і --\n",
      "своїми --\n",
      "діями --\n",
      ", --\n",
      "щоденною --\n",
      "працею --\n",
      ", --\n",
      "а --\n",
      "не --\n",
      "галасуванням --\n",
      ", --\n",
      "наближає --\n",
      "його --\n",
      "втілення --\n",
      "» --\n",
      ". --\n",
      "</S> --\n",
      "<S> --\n",
      "БУЛО --\n",
      "І --\n",
      "Є --\n",
      "</S> --\n",
      "<S> --\n",
      "У --\n",
      "1991-му --\n",
      "цей --\n",
      "об’єкт --\n",
      "був --\n",
      "реконструйований --\n",
      "і --\n",
      "в --\n",
      "1998 --\n",
      "році --\n",
      "внесений --\n",
      "до --\n",
      "переліку --\n",
      "пам’яток --\n",
      "історії --\n",
      ". --\n",
      "</S> --\n",
      "<S> --\n",
      "А --\n",
      "у --\n",
      "2006-му --\n",
      "цей --\n",
      "майданчик --\n",
      "продали --\n",
      ", --\n",
      "незважаючи --\n",
      "на --\n",
      "протести --\n",
      "громадськості --\n",
      ". --\n",
      "</S> --\n",
      "<S> --\n",
      "Захищати --\n",
      "велотрек --\n",
      "виходили --\n",
      "сотні --\n",
      "киян --\n",
      ", --\n",
      "і --\n",
      "це --\n",
      "були --\n",
      "фактично --\n",
      "перші --\n",
      "масові --\n",
      "протести --\n",
      "городян --\n",
      "проти --\n",
      "свавілля --\n",
      "забудовників --\n",
      "у --\n",
      "столиці --\n",
      ". --\n",
      "</S> --\n",
      "<S> --\n",
      "Поряд --\n",
      "побудували --\n",
      "висотку --\n",
      ", --\n",
      "яку --\n",
      "досі --\n",
      "не --\n",
      "можуть --\n",
      "ввести --\n",
      "в --\n",
      "експлуатацію --\n",
      ", --\n",
      "а --\n",
      "на --\n",
      "місці --\n",
      "чаші --\n",
      "велотреку --\n",
      "забудовник --\n",
      "планував --\n",
      "звести --\n",
      "офісний --\n",
      "центр --\n",
      "із --\n",
      "підземним --\n",
      "паркінгом --\n",
      ". --\n",
      "</S> --\n",
      "<S> --\n",
      "Питання --\n",
      "повернення --\n",
      "міського B-ORG\n",
      "велотреку I-ORG\n",
      "для --\n",
      "киян --\n",
      ", --\n",
      "і --\n",
      "особливо --\n",
      "шевченківців --\n",
      ", --\n",
      "було --\n",
      "принциповим --\n",
      ". --\n",
      "</S> --\n",
      "<S> --\n",
      "У --\n",
      "листопаді --\n",
      "минулого --\n",
      "року --\n",
      "столична --\n",
      "влада --\n",
      "повернула --\n",
      "Київський B-ORG\n",
      "велотрек I-ORG\n",
      "у --\n",
      "власність --\n",
      "громади --\n",
      "міста --\n",
      "— --\n",
      "депутати --\n",
      "Київради B-ORG\n",
      "проголосували --\n",
      "за --\n",
      "розірвання --\n",
      "договору --\n",
      "оренди --\n",
      "земельної --\n",
      "ділянки --\n",
      ", --\n",
      "укладеного --\n",
      "між --\n",
      "Київрадою B-ORG\n",
      "та --\n",
      "підприємством --\n",
      "« --\n",
      "Велотрек B-ORG\n",
      "Авангард I-ORG\n",
      "» --\n",
      ", --\n",
      "на --\n",
      "якій --\n",
      "розташовується --\n",
      "велотрек --\n",
      ". --\n",
      "</S> --\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip(tokens, labels):\n",
    "    print(i[0], j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract list of files for training and testing\n",
    "\n",
    "dev_test = {\"dev\": [], \"test\": []}\n",
    "category = \"\"\n",
    "with open(PATH + \"doc/dev-test-split.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        if line in [\"DEV\", \"TEST\"]:\n",
    "            category = line.lower()\n",
    "        elif len(line) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            dev_test[category].append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156 73\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_test[\"dev\"]), len(dev_test[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and test data and labels\n",
    "\n",
    "train_tokens, test_tokens, train_labels, test_labels = [], [], [], []\n",
    "\n",
    "for filename in dev_test[\"dev\"]:\n",
    "    try:\n",
    "        tokens = read_tokens(PATH + \"data/\" + filename + \".txt\")\n",
    "        train_tokens += [token[0] for token in tokens]\n",
    "        train_labels += extract_labels(read_annotations(PATH + \"data/\" + filename + \".ann\"), tokens)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for filename in dev_test[\"test\"]:\n",
    "    try:\n",
    "        tokens = read_tokens(PATH + \"data/\" + filename + \".txt\")\n",
    "        test_tokens += [token[0] for token in tokens]\n",
    "        test_labels += extract_labels(read_annotations(PATH + \"data/\" + filename + \".ann\"), tokens)\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "import nltk\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "from bpemb import BPEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpemb_uk = BPEmb(lang=\"uk\", dim=100)\n",
    "\n",
    "def calc_emb(text):\n",
    "    res = np.zeros(bpemb_uk.vectors.shape[1], dtype=np.float32)\n",
    "    # tokens = word_tokenize(text)\n",
    "    # for t in tokens:\n",
    "    embs = bpemb_uk.embed(text)\n",
    "    for e in embs:\n",
    "        res += e\n",
    "    n = len(embs)\n",
    "    if n:\n",
    "        res /= n\n",
    "    return res/2\n",
    "\n",
    "def word2features(tokens, labels, i):\n",
    "    word = tokens[i]\n",
    "    # print(word)\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word[:-3]': word[:-3],\n",
    "        'word[:-2]': word[:-2],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "    #emb = calc_emb(word)\n",
    "    #emb_features = {f'e{k}':v for k, v in enumerate(emb)}\n",
    "    #features.update(emb_features)\n",
    "    if i > 0 and tokens[i-1]!='<S>':\n",
    "        word1 = tokens[i-1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:label': labels[i-1],\n",
    "            '-1:qstart': word1=='«',\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "        features['-1:label']='--'\n",
    "\n",
    "    if i < len(tokens)-1 and tokens[i+1]!='</S>':\n",
    "        word1 = tokens[i+1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:qend': word1=='»',\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def tokens2features(tokens, labels):\n",
    "    return [word2features(tokens, labels, i) for i in range(len(tokens))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bias': 1.0,\n",
       "  'word.lower()': '<s>',\n",
       "  'word[-3:]': '<S>',\n",
       "  'word[-2:]': 'S>',\n",
       "  'word[:-3]': '',\n",
       "  'word[:-2]': '<',\n",
       "  'word.isupper()': True,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'BOS': True,\n",
       "  '-1:label': '--',\n",
       "  '+1:word.lower()': 'на',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:qend': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'на',\n",
       "  'word[-3:]': 'На',\n",
       "  'word[-2:]': 'На',\n",
       "  'word[:-3]': '',\n",
       "  'word[:-2]': '',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'BOS': True,\n",
       "  '-1:label': '--',\n",
       "  '+1:word.lower()': 'довірливих',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:qend': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'довірливих',\n",
       "  'word[-3:]': 'вих',\n",
       "  'word[-2:]': 'их',\n",
       "  'word[:-3]': 'довірли',\n",
       "  'word[:-2]': 'довірлив',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  '-1:word.lower()': 'на',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:label': '--',\n",
       "  '-1:qstart': False,\n",
       "  '+1:word.lower()': 'кіровоградців',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:qend': False}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emb = calc_emb('слово')\n",
    "# print(emb)\n",
    "# emb_features = {f'e{k}':v for k, v in enumerate(emb)}\n",
    "\n",
    "tokens2features(train_tokens, train_labels)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [tokens2features(train_tokens, train_labels)]\n",
    "y_train = [train_labels]\n",
    "\n",
    "X_test = [tokens2features(test_tokens, test_labels)]\n",
    "y_test = [test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "loading training data to CRFsuite:   0%|                                                         | 0/1 [00:00<?, ?it/s]\n",
      "loading training data to CRFsuite: 100%|█████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 170075\n",
      "Seconds required: 0.638\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.100000\n",
      "c2: 0.100000\n",
      "num_memories: 6\n",
      "max_iterations: 100\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=0.34  loss=131442.13 active=169619 feature_norm=1.00\n",
      "Iter 2   time=0.17  loss=69777.51 active=167405 feature_norm=2.94\n",
      "Iter 3   time=0.17  loss=62839.88 active=164458 feature_norm=2.74\n",
      "Iter 4   time=0.50  loss=45256.84 active=156919 feature_norm=2.26\n",
      "Iter 5   time=0.17  loss=39771.95 active=147869 feature_norm=2.62\n",
      "Iter 6   time=0.17  loss=32431.76 active=85497 feature_norm=4.54\n",
      "Iter 7   time=0.17  loss=27277.89 active=83040 feature_norm=4.95\n",
      "Iter 8   time=0.18  loss=23400.72 active=84121 feature_norm=5.74\n",
      "Iter 9   time=0.17  loss=20347.11 active=75742 feature_norm=6.42\n",
      "Iter 10  time=0.17  loss=17188.32 active=71284 feature_norm=7.66\n",
      "Iter 11  time=0.20  loss=13117.24 active=62967 feature_norm=10.19\n",
      "Iter 12  time=0.19  loss=11551.19 active=59163 feature_norm=12.21\n",
      "Iter 13  time=0.17  loss=10403.58 active=58475 feature_norm=12.85\n",
      "Iter 14  time=0.17  loss=9800.38  active=54892 feature_norm=14.28\n",
      "Iter 15  time=0.18  loss=9128.04  active=51211 feature_norm=15.43\n",
      "Iter 16  time=0.19  loss=8448.97  active=49287 feature_norm=16.94\n",
      "Iter 17  time=0.18  loss=7735.38  active=47083 feature_norm=19.09\n",
      "Iter 18  time=0.20  loss=7158.26  active=46398 feature_norm=21.09\n",
      "Iter 19  time=0.19  loss=6684.28  active=45839 feature_norm=22.02\n",
      "Iter 20  time=0.18  loss=5901.33  active=43066 feature_norm=25.11\n",
      "Iter 21  time=0.18  loss=5599.13  active=36979 feature_norm=28.73\n",
      "Iter 22  time=0.17  loss=5040.36  active=35557 feature_norm=30.21\n",
      "Iter 23  time=0.17  loss=4757.13  active=33578 feature_norm=32.15\n",
      "Iter 24  time=0.18  loss=4342.95  active=30660 feature_norm=36.50\n",
      "Iter 25  time=0.17  loss=4014.04  active=29900 feature_norm=41.03\n",
      "Iter 26  time=0.17  loss=3732.30  active=29891 feature_norm=42.99\n",
      "Iter 27  time=0.18  loss=3508.89  active=28891 feature_norm=45.66\n",
      "Iter 28  time=0.20  loss=3184.43  active=27747 feature_norm=51.63\n",
      "Iter 29  time=0.19  loss=2982.50  active=27328 feature_norm=55.95\n",
      "Iter 30  time=0.18  loss=2847.92  active=27292 feature_norm=57.34\n",
      "Iter 31  time=0.17  loss=2664.07  active=25978 feature_norm=60.82\n",
      "Iter 32  time=0.17  loss=2525.40  active=25520 feature_norm=63.68\n",
      "Iter 33  time=0.17  loss=2435.51  active=25040 feature_norm=65.11\n",
      "Iter 34  time=0.17  loss=2345.84  active=24311 feature_norm=66.90\n",
      "Iter 35  time=0.17  loss=2270.77  active=23446 feature_norm=68.37\n",
      "Iter 36  time=0.17  loss=2226.37  active=22460 feature_norm=69.56\n",
      "Iter 37  time=0.17  loss=2184.90  active=21795 feature_norm=70.10\n",
      "Iter 38  time=0.17  loss=2147.64  active=20721 feature_norm=71.01\n",
      "Iter 39  time=0.17  loss=2115.79  active=19666 feature_norm=71.73\n",
      "Iter 40  time=0.17  loss=2093.65  active=19019 feature_norm=72.54\n",
      "Iter 41  time=0.18  loss=2078.24  active=18684 feature_norm=72.95\n",
      "Iter 42  time=0.17  loss=2062.71  active=18242 feature_norm=74.15\n",
      "Iter 43  time=0.17  loss=2050.10  active=18116 feature_norm=74.24\n",
      "Iter 44  time=0.17  loss=2039.89  active=17849 feature_norm=74.48\n",
      "Iter 45  time=0.17  loss=2029.31  active=17093 feature_norm=75.18\n",
      "Iter 46  time=0.17  loss=2017.18  active=16876 feature_norm=75.32\n",
      "Iter 47  time=0.18  loss=2010.62  active=16706 feature_norm=75.57\n",
      "Iter 48  time=0.17  loss=2005.04  active=16379 feature_norm=75.75\n",
      "Iter 49  time=0.17  loss=1999.01  active=16182 feature_norm=76.04\n",
      "Iter 50  time=0.17  loss=1994.68  active=16050 feature_norm=76.15\n",
      "Iter 51  time=0.17  loss=1989.58  active=15961 feature_norm=76.32\n",
      "Iter 52  time=0.17  loss=1985.78  active=15803 feature_norm=76.40\n",
      "Iter 53  time=0.18  loss=1982.16  active=15688 feature_norm=76.49\n",
      "Iter 54  time=0.17  loss=1978.31  active=15553 feature_norm=76.57\n",
      "Iter 55  time=0.17  loss=1975.35  active=15426 feature_norm=76.66\n",
      "Iter 56  time=0.20  loss=1972.44  active=15332 feature_norm=76.67\n",
      "Iter 57  time=0.19  loss=1970.51  active=15246 feature_norm=76.71\n",
      "Iter 58  time=0.18  loss=1968.12  active=15167 feature_norm=76.69\n",
      "Iter 59  time=0.17  loss=1966.02  active=15084 feature_norm=76.70\n",
      "Iter 60  time=0.17  loss=1964.31  active=14941 feature_norm=76.66\n",
      "Iter 61  time=0.17  loss=1962.20  active=14839 feature_norm=76.70\n",
      "Iter 62  time=0.17  loss=1960.23  active=14786 feature_norm=76.67\n",
      "Iter 63  time=0.17  loss=1958.56  active=14743 feature_norm=76.70\n",
      "Iter 64  time=0.18  loss=1956.98  active=14670 feature_norm=76.69\n",
      "Iter 65  time=0.17  loss=1955.45  active=14615 feature_norm=76.74\n",
      "Iter 66  time=0.17  loss=1954.27  active=14511 feature_norm=76.73\n",
      "Iter 67  time=0.17  loss=1952.45  active=14450 feature_norm=76.79\n",
      "Iter 68  time=0.17  loss=1950.99  active=14396 feature_norm=76.80\n",
      "Iter 69  time=0.17  loss=1949.74  active=14356 feature_norm=76.85\n",
      "Iter 70  time=0.18  loss=1948.62  active=14298 feature_norm=76.87\n",
      "Iter 71  time=0.17  loss=1947.70  active=14221 feature_norm=76.92\n",
      "Iter 72  time=0.17  loss=1946.54  active=14141 feature_norm=76.94\n",
      "Iter 73  time=0.17  loss=1945.54  active=14096 feature_norm=76.99\n",
      "Iter 74  time=0.17  loss=1944.38  active=14048 feature_norm=77.00\n",
      "Iter 75  time=0.17  loss=1943.60  active=13998 feature_norm=77.05\n",
      "Iter 76  time=0.17  loss=1942.38  active=13955 feature_norm=77.06\n",
      "Iter 77  time=0.17  loss=1941.71  active=13933 feature_norm=77.12\n",
      "Iter 78  time=0.17  loss=1940.63  active=13893 feature_norm=77.13\n",
      "Iter 79  time=0.17  loss=1939.97  active=13851 feature_norm=77.19\n",
      "Iter 80  time=0.17  loss=1938.84  active=13826 feature_norm=77.19\n",
      "Iter 81  time=0.17  loss=1938.20  active=13781 feature_norm=77.25\n",
      "Iter 82  time=0.18  loss=1937.11  active=13748 feature_norm=77.27\n",
      "Iter 83  time=0.17  loss=1936.49  active=13723 feature_norm=77.32\n",
      "Iter 84  time=0.17  loss=1935.60  active=13688 feature_norm=77.33\n",
      "Iter 85  time=0.17  loss=1935.09  active=13650 feature_norm=77.38\n",
      "Iter 86  time=0.17  loss=1934.02  active=13617 feature_norm=77.40\n",
      "Iter 87  time=0.18  loss=1933.60  active=13578 feature_norm=77.44\n",
      "Iter 88  time=0.20  loss=1932.58  active=13544 feature_norm=77.44\n",
      "Iter 89  time=0.17  loss=1932.35  active=13492 feature_norm=77.50\n",
      "Iter 90  time=0.18  loss=1931.22  active=13453 feature_norm=77.50\n",
      "Iter 91  time=0.19  loss=1930.71  active=13414 feature_norm=77.53\n",
      "Iter 92  time=0.19  loss=1929.89  active=13379 feature_norm=77.53\n",
      "Iter 93  time=0.18  loss=1929.65  active=13335 feature_norm=77.57\n",
      "Iter 94  time=0.18  loss=1928.50  active=13297 feature_norm=77.56\n",
      "Iter 95  time=0.18  loss=1928.12  active=13284 feature_norm=77.60\n",
      "Iter 96  time=0.18  loss=1927.24  active=13249 feature_norm=77.59\n",
      "Iter 97  time=0.17  loss=1926.99  active=13220 feature_norm=77.62\n",
      "Iter 98  time=0.17  loss=1926.11  active=13186 feature_norm=77.61\n",
      "Iter 99  time=0.17  loss=1925.99  active=13169 feature_norm=77.65\n",
      "Iter 100 time=0.18  loss=1925.02  active=13127 feature_norm=77.64\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 18.103\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 13127 (170075)\n",
      "Number of active attributes: 9544 (149604)\n",
      "Number of active labels: 9 (9)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.031\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=0.1, c2=0.1,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=1)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    "    verbose=1\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC', 'B-PERS', 'I-PERS']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(crf.classes_)\n",
    "labels.remove('--')\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8738412312645141"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred, \n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.831     0.751     0.789       413\n",
      "       I-LOC      0.967     0.768     0.856      1071\n",
      "      B-MISC      0.646     0.580     0.611       176\n",
      "      I-MISC      0.970     0.939     0.954       375\n",
      "       B-ORG      0.615     0.482     0.541       228\n",
      "       I-ORG      0.991     0.715     0.831      1954\n",
      "      B-PERS      0.843     0.784     0.812      1155\n",
      "      I-PERS      0.979     0.985     0.982      2795\n",
      "\n",
      "   micro avg      0.935     0.826     0.878      8167\n",
      "   macro avg      0.855     0.750     0.797      8167\n",
      "weighted avg      0.936     0.826     0.874      8167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    labels, \n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
