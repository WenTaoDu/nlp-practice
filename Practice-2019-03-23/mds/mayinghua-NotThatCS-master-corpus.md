# 语料库

建立样本库是数据挖掘、人工智能研究不可缺少的基础工作。

语料库是指以文本类型为样本的数据集。有的语料库重在数量，有的则关注加工。

推荐资料：

图片分类的TED(李飞飞) https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures?language=zh-cn

儿童习得语言的资料TED记录：单词的诞生http://open.163.com/movie/2011/7/E/J/M77TUGO7R_M77TUSJEJ.html

## 语料库列表
- 语料库在线 http://www.cncorpus.org/
- 现代汉语语料库 http://ccl.pku.edu.cn/corpus.asp?item=1
- 古代汉语语料库 http://ccl.pku.edu.cn/corpus.asp?item=2
- 汉英双语语料库 http://ccl.pku.edu.cn/corpus.asp?item=3
- HSK动态作文语料库 http://202.112.195.192:8060/hsk/login.asp
- 北京口语语料查询系统 http://www.blcu.edu.cn/yys/6_beijing/6_beijing_chaxun.asp
- 现代汉语平衡语料库 http://rocling.iis.sinica.edu.tw/new/20corpus.htm
- LIVAC共時語料庫 http://www.livac.org/index.php
- 兰开斯特汉语语料库 http://ling.cass.cn/dangdai/LCMC/LCMC.htm
- 洛杉矶加州大学汉语语料库 http://www.lancs.ac.uk/fass/projects/corpus/UCLA/
- 中文新闻分类语料库 http://www.nlpir.org/?action-viewnews-itemid-145
- NLPIR 500万条twitter内容语料库 http://www.nlpir.org/?action-viewnews-itemid-263
- NLPIR微博博主语料库100万条 http://www.nlpir.org/?action-viewnews-itemid-232
- 現代漢語語料庫詞頻統計 http://elearning.ling.sinica.edu.tw/CWordfreq.html
- 欢迎关注新浪微博【对外汉语北京】
- 中文句結構樹資料庫 http://turing.iis.sinica.edu.tw/treesearch/
- 搜狗文本分类语料库 http://www.sogou.com/labs/dl/c.html
- 哈工大信息检索研究室对外共享语料库 http://ir.hit.edu.cn/demo/ltp/Sharing_Plan.htm
- 传媒大学文本语料库 http://ling.cuc.edu.cn/RawPub/
- 词语研究资源库 对外汉语北京 http://ling.cuc.edu.cn/newword/web/index.asp
- BFSU CQPweb多语言在线语料库检索平台 http://www.iresearch.ac.cn/paper/detail.php?ItemID=6358
- 英汉双语平行语料库 http://www.luweixmu.com/ec-corpus/
- babel 汉英平行语料库 http://icl.pku.edu.cn/icl_groups/parallel/default.htm
- 中国法律法规汉英平行语料库（大陆） http://corpus.zscas.edu.cn/lawcorpus1/index.asp
- 国家语言资源监测与研究中心 http://www.clr.org.cn/
- BCC语料库 http://bcc.blcu.edu.cn/
- 北外语料库语言学 http://www.bfsu-corpus.org/
- 美国当代英语语料库 http://corpus.byu.edu/coca/

## 样本收集方法

## 语料库加工方法

### 词语切分

#### 汉语自动分词技术

1. 概念
通过计算机把组成汉语文本的字串自动转换为词串的过程
2. 目的
    - TTS或语音合成： 只有正确切词，才能知道正确的发音
    - 信息检索： 分词有助于提高信息检索的准确率
    - 词语的计量分析：词频统计等
    - 深层汉语分析的基础：句法分析、语义分析等
3. 基本方法
    - 最大匹配法(MM)
        - 正向最大匹配法(MM)
        - 逆向最大匹配法(RMM)
4. 评价标准
    - 准确率(precision)
      准确率（P）＝切分结果中正确分词数/切分结果中所有分词数*100%
    - 召回率(recall)
      召回率（R）＝切分结果中正确分词数/标准答案中所有分词数*100%
    - F-评价(F-measure综合准确率和召回率的评价指标)
      F-指标＝2PR/(P+R)
5. 关键问题
    - 切分歧义（消解）：一个字串有不止一种切分结果
    - 未登录词识别：专有名词、新词

### 词性标注

所谓词性标注(Part of Speech tagging)就是根据句子上下文中的信息给句中的每个词一个正确的词性标记，即确定每个词的词性是名词、动词、形容词或者是其他词性。词性标注主要是针对多标记词 (即有多种词性的词)和未登录词(即在训练语料中未出现的词)进行的。

词性标注是自然语言处理领域的基础，可以提高信息检索的效果和效率，它在信息检索领域有着非常重要的作用。国内外该方面研究人员很重视它，成功设计出很多词性标注模型。归纳起来，比较典型的标注算法有： 

(1)基于规则的方法

基于规则的标注系统与系统设计者的语言能力有关，规则集直接体现了设计者的语言能力。不幸的是，要对某一种语言的各种语言现象都构造规则的话，将是一项很艰难也很耗时的任务。基于规则的标注系统另一个常见问题是：当根据规则判断一个词的词性时可能面临多种选择，如果不根据上下文则很难做出正确的选择。

(2)基于统计的方法 

20世纪80年代初，随着经验主义方法在计算语言学中的重新崛起，统计方法在词性标注中占据了领导地位。对于给定的输入词串，基于统计的方法先确定其所有可能的词性串，然后对它们分别打分，并选出得分最高的词性串作为最佳的输出。常见的方法有基于N元模型的方法和基于隐马尔科夫模型的方法。

参考：王丽杰, 车万翔, 刘挺. 基于SVMTool的中文词性标注[J]. 中文信息学报, 2009, 23(4):16-21.

### 命名主体识别

命名主体识别是指识别句子中特定类别的词语，包括但不限于：地名（Location）、人物（Person）、组织（Organization）、政治实体（Geo Political Entity，GPE）和杂项（MISC）。目前比较有效的命名主体识别的算法是基于条件随机场（CRF）的算法。

### 文本关键字提取算法

文本关键字提取是指对于一段文本，提取出一组关键字来表征这段文本。最常用也是最简单的一种提取算法是TF-IDF算法：

    TF-IDF算法全名Term Frequency-Inverse Document Frequency，是一种基于词频统计的算法，TF就代表词频，IDF可以视作一种权重。TF-IDF算法在中文自动摘要、信息检索、关键字提取等各种领域使用非常广泛。
    TF-IDF算法的步骤如下：
		第一步，计算TF。统计某个词在文章中的出现次数，然后除以文章的总词数，得到标准化的词频。如果只是文章内比较，可以不用除总词数。
			TF(s)=s在文中出现的总次数/文章的总词数 ,	
			或 TF(s)= s在文中出现的总次数	
		第二步，计算IDF。通常这是根据一个语料库预先就计算好的，计算公式有很多种，这里采用非平滑化的公式:
			IDF(s)=log⁡(语料库文档总数/(出现了s的文档数+1))	
		第三步，计算TF-IDF值，公式如下：
			TF-IDF(s)=TF(s)*IDF(s)	
		第四步，按照降序排序TF-IDF的值，取排在前面的前几个词。


## 其他相关内容
### 构建语料库的原则
语料库应该具有代表性、结构性、平衡性、规模需求并制定语料的元数据规范，各个原则具体介绍如下：

* 代表性：在应用领域中，不是根据量而划分是否是语料库，而是在一定的抽样框架范围内采集而来的，并且在特定的抽样框架内做到代表性和普遍性。
* 结构性：有目的的收集语料的集合，必须以电子形式存在，计算机可读的语料集合结构性体现在语料库中语料记录的代码，元数据项、数据类型、数据宽度、取值范围、完整性约束。
* 平衡性：主要体现在平缓因子：学科、年代、文体、地域、登载语料的媒体、使用者的年龄、性别、文化背景、阅历、预料用途（私信/广告等），根据实际情况选择其中一个或者几个重要的指标作为平衡因子，最常见的平衡因子有学科、年代、文体、地域等。
* 规模性：大规模的语料对语言研究特别是对自然语言研究处理很有用的，但是随着语料库的增大，垃圾语料越来越多，语料达到一定规模以后，语料库功能不能随之增长，语料库规模应根据实际情况而定。
* 元数据：元数据对于研究语料库有着重要的意义，我们可以通过元数据了解语料的时间、地域、作者、文本信息等；还可以构建不同的子语料库；除此外，还可以对不同的子语料对比；另外还可以记录语料知识版权、加工信息、管理信息等。

参考：[【NLP】大数据之行，始于足下：谈谈语料库知多少](http://www.cnblogs.com/baiboy/p/ylk.html)

### NLTK库及其包含的语料库介绍
	
TF-IDF(s)=TF(s)*IDF(s)	(3-23)
NLTK是斯坦福大学开发的处理自然语言的python库，主要包含如下功能：

* 语料库：提供了经典书籍，词典，演讲，网络语言，论坛等各种语料库；
* 断句与分词：可以方便地对文章，段落进行分词；
* 词频统计：计算句子或者文章中每个单词的频率；
* 同义词与词态：单词的同义词「WordNet」和单词词态「过去式，进行时等」的还原；
* 词性的标注：动词，名词，形容词和副词等的标注；
* 分类算法：例如常见的信息熵，朴素贝叶斯算法，最大信息熵模型等；

许多英文自然语言处理的研究都建立在NLTK提供的语料库上，其中比较常用有以下几种：

* `gutenberg`：古登堡语料库，古登堡计划致力于将文化作品的数字化和归档，古登堡语料库选择了 部分文本，包含了一百七十万字。
* `webtext`：网络文本语料库，网络和聊天文本
* `brown`：布朗语料库，按照文本分类好的500个不同来源的文本
* `reuters`：路透社语料库，1万多个新闻文档
* `inaugural`：就职演说语料库，55个总统的演说

安装`NLTK库`后利用`from nltk.corpus import <语料名>`即可导入相应的语料库。此外，还有一些仅是词或短语以及一些相关信息的集合，叫做词典资源。

* 词汇列表语料库：`nltk.corpus.words.words()`，所有英文单词，可以用来识别语法错误
* 停用词语料库：`nltk.corpus.stopwords.words()`，用来识别那些最频繁出现的信息量较小的词
* 发音词典：`nltk.corpus.cmudict.dict()`，用来输出每个英文单词的发音
* 比较词表：`nltk.corpus.swadesh()`，多种语言核心200多个词的对照，可以作为语言翻译的基础
* 同义词集：WordNet，面向语义的英语词典，由同义词集组成，并组织成一个网络

参考：
[NLTK笔记：简介与环境搭建](http://blog.ourren.com/2015/02/05/nltk_note_environment_install/)

[自己动手做聊天机器人 三-语料与词汇资源](http://www.shareditor.com/blogshow/?blogId=65)


### 维基百科中文语料库

维基百科的英文语料库因其高质量早已被广泛运用，其提供了中文版的语料库后，
逐渐在中文自然语言处理中广泛使用，其资源获取非常方便，可直接在[Wiki Dump](https://dumps.wikimedia.org/zhwiki/)上下载，其更新速度也很快，最新一次备份时间为2016年10月1日。虽然中文维基百科中的文字均是繁体，但是在经过简单的繁体到简体转换就能方便地使用，对该语料库文档的解析有非常多的成熟工具，直接使用开源工具即可完成正文的提取。

对于该语料库的使用方式和应用场景也在多篇博文中提到：

* [中英文维基百科语料上的Word2Vec实验](http://www.52nlp.cn/中英文维基百科语料上的word2vec实验)
* [维基百科简体中文语料的获取](http://licstar.net/archives/262)
* [用wiki百科中文语料训练word2vec模型](http://blog.csdn.net/hereiskxm/article/details/49664845)

参考：
[自然语言处理之语料库资源](http://blog.just4fun.site/NLP-corpus.html)

### 搜狗实验室数据资源 （http://www.sogou.com/labs/）  
	1. 包含了5大类13种数据资源  
		1.1 评测集合   
			1.1.1 搜索结果评价   
				简介：判断搜索结果与查询的相关性，是否符合搜索意图。  
				数据：完整版4326条，数据格式“查询词\t相关的URL\t查询类别”。  
				相关任务：基于互联网语料的信息检索。  
				相关技术：2.1。   
			1.1.2 话题跟踪及检测评价   
				简介：评测新闻话题跟踪及检测效果。  
				数据：完整版953条，数据格式“URL\t话题名称”。  
				相关任务：文本分类。  
				相关技术：2.2  
			1.1.3 文本分类评价   
				简介：评估文本分类结果的正确性。    
				数据：94条，数据格式“URL前缀\t对应类别标记”。  
				相关任务：文本分类。  
		1.2 语料数据   
			1.2.1 互联网语料库  
				简介：来自互联网各种类型的1.3亿个原始网页。  
				数据：完整版1TB，迷你版10个页面数据。  
				相关任务：相关性排序，文本分类，新词发现，机器翻译，分词。  
				相关技术：2.3，2.4      
			1.2.2 链接关系库  
				简介：包括对应互联网语料库内文档的链接关系列表。  
				数据：完整版90GB，迷你版1000条URL对照表和1000条链接关系。  
				相关任务：相关行排序，链接分析，反垃圾。  
				相关技术：2.4  
			1.2.3 SogouRank库  
				简介：互联网语料库中各页面的重要程度评级。  
				数据：完整版90GB，迷你版1001条，数据格式“URL\tSougouRank”。  
				相关任务：相关性排序。  
			1.2.4 用户查询日志  
				简介：搜索引擎部分网页查询需求及用户点击情况的网页查询日志数据。  
				数据：完整版1.9GB，迷你版10000条，数据格式“访问时间\t用户ID\t[查询词]\t该URL在返回结果中的排名\t用户点击的顺序号\t用户点击的URL”。  
				相关任务：相关行排序，用户兴趣挖掘，查询扩展，新词发现。  
		1.3 新闻数据  
			1.3.1 全网新闻数据  
				简介：来自5个新闻站点共83个频道的新闻数据，提供URL和正文信息。  
				数据：完整版1.02GB，迷你版200条新闻数据。  
				相关任务：文本分类，事件检测跟踪，新词发现，命名实体识别，自动摘要。  
				相关技术：2.2  
			1.3.2 搜狐新闻数据  
				简介：来自搜狐新闻共18个频道的新闻数据，提供URL和正文信息。  
				数据：完整版65GB，迷你版200条新闻数据，论文（见2.2）版953条。 
				相关任务：文本分类，事件检测跟踪，新词发现，命名实体识别，自动摘要。  
				相关技术：2.2  
		1.4 图片数据  
			1.4.1 互联网图片库  
				简介：来自sougou图片搜索索引的280多万张抓取图片及标注数据集合。    
				数据：完整版269GB。  
				相关任务：基于文本/内容的图片检索。  
			1.4.2 互联网图片库2.0  
				简介：1000万张互联网图片和相关文本信息，以及识图搜索结果的人工标注集合。    
				数据：完整版635GB。  
				相关任务：基于内容的图片检索。  
		1.5 自然语言处理相关数据   
			1.5.1 互联网词库  
				简介：基于互联网语料环境的高频词对应的词频、词性信息。      
				数据：完整版1.3MB，共157202个词。  
				相关任务：中文词性标注，词频分析。  
			1.5.2 中文词语搭配库  
				简介：基于互联网语料的字词搭配关系统计。        
				数据：完整版共18399496组，迷你版共100000组。  
				相关任务：中文输入法，文字到语音转化，语音识别。  
				相关技术：2.5，2.6  
	2. 提出了一些相关技术并进行了实验  
		2.1 基于点击数据分析，自动对搜索结果进行评估。(http://www.sogou.com/labs/paper/Automatic_Search_Engine_Performance_Evaluation_with_Click-through_Data_Analysis.pdf)   
			- 将搜索分成三种情况：找特定网址、找信息、处理事务。“找特定网址”具有明确的目标特征。因此这里仅考虑这类搜索情况。  
			- 通过统计搜索词q与搜索结果中点击量最多的链接r，统计（q，r），并在下次给出搜索结果时让r更靠前，如此不断验证。  
			- 从2006年6月至2007年1月，对sougou.com的检索和点击数据进行统计。  
			- 准确率为97%左右。错误结果中，一些网址是正确网址的子网站。例如，搜索163通常会定位到163邮箱mail.163.com，而非www.163.com。        
		2.2 网络环境下自动进行在线新闻事件的生成。(http://www.sogou.com/labs/paper/Automatic_Online_News_Issue_Construction_in_Web_Environment.pdf)    
			- 方法分三步：  
				1）话题检测，将新出现的文本内容聚类成候选话题。  
				2）话题对比，候选话题与既有的话题比较，并入既有话题或称为新的话题。  
				3）根据相关话题生成新闻事件。  
			- 具体做法：   
				1）预处理：提取页面内容，分句，除去无用句，对单词进行标记（中文进行词汇划分），词性标注，识别命名实体，删除“应删除词”（例如“的”），最终每篇文章生成一个词向量。  
				2）计算t时刻词w的“词频”，每篇文章表示表示为t时刻的一个n维向量，使用增量TF-IWF模型计算各维度权重weight t(d,w)并归一化处理。  
				3）使用余弦相似度算法计算两篇文章间的相似度。  
				4）使用算术平均加权配对组（UPGMA）的聚类方法，将新文章聚类至候选话题中。    
				5）对候选话题与既有话题相比较，并入并更新既有话题，或成为新的话题。  
				6）类似地将各话题聚类到新闻事件中，并不断更新新闻事件或生成新的新闻事件。    
				7）自动对各新闻事件加入网络上的相关博文、评论和图片、音频、视频等。 
			- 实验数据：  
				数据集1：含350个新闻页面，87个话题。2007年3月到4月，均关于搜索引擎公司。中文新闻网站，包括新闻报道和新闻回顾。最多的话题包含20个文章，最少1个。  
				数据集2：含953个新闻页面，108个话题。2007年4月22日的搜狐体育新闻频道。最大话题有151个文章，最少1个。  
				数据集3：含24872个新闻页面，选自多个中文新闻网站和1339篇博文和评论文章。  
			- 实验结果：  
				1）聚类方法的选择：direct-I2-IDF、direct-I2-IWF、rbr-H2-IDF、rbr-H2-IWF、graph-jacc-IDF、graph-jacc-IWF、agglo-upgma-IDF、agglo-upgma-IWF这八个算法中，“aggloupgma-IWF”算法更优。  
				2）IWF与IDF比较：IWF模型的效果更平滑更优。  
				3）冗余句子的去除（RSR）：新闻中冗余句子较少，RSR有效果但不明显。  
				4）标题的使用及权重：加标题表现更好；仅短文加标题比全部加标题更好；标题权重比正文相对更大表现更好。    
		2.3 使用与查询无关的特征对网络信息检索数据进行清洗。(http://www.sogou.com/labs/paper/Data_Cleansing_for_Web_Information_Retrieval_using_Query_Independent_Features.pdf)  
			- 若使用普通的链接分析方法，基于网页的被点击概率，而非页面的有用度。因此使用与查询无关的特征。  
			- 结合利用了链接分析和页面布局分析，进行全局规模的数据清洗。  
			- 使用朴素贝叶斯学习算法，在低维度实例空间上高效实用，且不需要原有数据集的先验知识。  
			- 将5个特征：文字长度，链接文本长度，搜索排名值，导入链接数量和导出链接数量综合应用，比仅使用单独一个特征效果更好。  
			- 清洗后选出的高质量页面占全部的52%（.GOV数据集）和5%（sogou数据集），召回率>90%。即牺牲了10%的正确搜索结果，大大节省了存储空间。  
			- 同时能够消除30%的垃圾页面和15%的低质量页面。  
		2.4 一种基于链接分析的垃圾页面检测算法。(http://www.sogou.com/labs/paper/R-SpamRank_A_Spam_Detection_Algorithm_Based_on_Link_Analysis.pdf)  
			- 一些人试图误导搜索引擎以提升网页的搜索排名，方法主要是基于内容和基于链接两种。这里提出一种半自动的检测基于链接的垃圾网页的方法。  
			- 使用人工识别的垃圾页面黑名单作为种子，设置高RSR值。根据链接到本页面的情况，反向传播RSR值，并不断迭代直到各页面值稳定。  
			- 测试数据为sogou.com的500万个网页，迭代50次。人工分析后，不能打开的页面赋值0，好的页面赋值1，半垃圾页面赋值2，纯垃圾页面赋值3。  
			- 将结果中RSR值最高的1万个页面中的前100个和最后100个取出做人工检查。发现99%是垃圾页面。证明算法有效。  
			- 发现垃圾页面集中在两个域名，说明算法能够检测链接工厂。  
			- 做域名清理，即删除3个链接工厂下的所有页面后再分析。剩下的178个页面中仍有87.1%的垃圾页面。  
			- 前5大链接工厂产生了99.1%的垃圾页面。  
		2.5 基于相对条件熵的搭配抽取方法。(http://www.sogou.com/labs/paper/Wangdaliang_JoBUPT_07.pdf)   
			- 在自然语言处理中，研究搭配组词项之间的内在倾向性。提出使用相对条件熵比传统的使用互信息评价二元相关性更优。  
			- 词语在语料库中出现越频繁，越容易失去倾向的特性。  
			- 绝大多数次的左搭配力大于自己的右搭配力。  
			- 左搭配倾向强的多是定语；右搭配倾向抢的多是宾语。  
			- 自然文本的搭配抽取方法：  
				1）预处理：提取文本，分词，词性标注，停用词过滤，词频过滤。    
				2）数据统计：利用词性过滤模板、滑动窗口生成搭配候选二元组统计矩阵，同时统计二元同现次数、构成次的词频以及样本总词频。  
				3）搭配抽取：计算候选二元组左、右搭配倾向强度以及搭配整合强度，依据阈值输出搭配抽取结果。  
			- 对sogou.com的1亿多个中文页面语料库数据进行实验。预处理得搭配候选二元组35万个，自动获取搭配词对3.5万个。人工验证得效果比互信息法好。  
		2.6 多策略融合的搭配抽取方法。(http://www.sogou.com/labs/paper/Wangdaliang_JoTHU_08.pdf)  
			- 搭配抽取中，需要识别频繁二元组和稀疏二元组，而排除无关二元组。  
			- 互信息法可作为二元组无关性的度量方法，用于排除大部分无关二元组。  
			- 卡方检验法比t检验法更适合于刻画二元组的相关性，且能很好识别频繁二元组，但对稀疏二元组不行。  
			- 对数似然比检验法可用于识别稀疏二元组。  
			- 对sogou.com的1亿多个中文页面语料库数据进行实验。预处理得搭配候选二元组35万个，自动获取搭配词对5万个。人工验证得多策略融合法效果更好。  
			
### NLP开源工具包
中文词法分析
THULAC：一个高效的中文词法分析工具包
http://thulac.thunlp.org/
包括中文分词、词性标注功能。已经提供C++、Java、Python版本。

中文文本分类
THUCTC: 一个高效的中文文本分类工具
http://thuctc.thunlp.org/
提供高效的中文文本特征提取、分类训练和测试功能。

THUTag: 关键词抽取与社会标签推荐工具包
https://github.com/YeDeming/THUTag/
提供关键词抽取、社会标签推荐功能，包括TextRank、ExpandRank、Topical PageRank（TPR）、Tag-LDA、Word Trigger Model、Word Alignment Model等算法。

PLDA / PLDA+: 一个高效的LDA分布式学习工具包
https://code.google.com/archive/p/plda/

知识表示学习
知识表示学习工具包
https://github.com/mrlyk423/relation_extraction
包括TransE、TransH、TransR、PTransE等算法。

考虑实体描述的知识表示学习算法
https://github.com/xrb92/DKRL

词表示学习
跨语言词表示学习算法
http://nlp.csai.tsinghua.edu.cn/~lzy/src/acl2015_bilingual.html

主题增强的词表示学习算法
https://github.com/largelymfs/topical_word_embeddings

可解释的词表示学习算法
https://github.com/SkTim/OIWE

考虑字的词表示学习算法
https://github.com/Leonard-Xu/CWE

网络表示学习
文本增强的网络表示学习算法
https://github.com/albertyang33/TADW

Natural Language Toolkit
http://www.nltk.org/

哈工大LTP：http://ir.hit.edu.cn/

### 语料库数据化开发
虽然数据库目的性和实用性更强，但需要大量人力物力的投入，发展比较缓慢。针对这种情况，西方语言学者就开始直接对语料库进行深加工处理，把语料库的建设从追求量的扩充向语料数据化方向转化，即利用标注、数据挖掘技术和计算机自动运算的方法对无序的语料索引行进行梳理、统计、分类、归纳，按用户需求有针对性地呈现各种典型的词汇数据。美国当代英语语料库( Corpus of Contemporary American English，以下简称 COCA) 就是一个典型的代表。

COCA由美国杨百翰大学( Brigham Young University) 的 Mark Davies教授主持开发，语料规模达4.5亿词，是美国目前最新的当代英语平衡语料库。自 2008 年 2 月 20 日在互联网上正式推出以来，每年都要至少做两次语料更新。语料库的数据化主要体现在将杂乱无序的语料变得有条有序，能根据用户的需要提供相对准确的词汇语言数据，其关键就是“智能化”索引，而索引的基础是对语料的标注和基础数据库的支持。下面就从这几个方面谈谈COCA数据化的功能特征。 

1 语料库的标注 

词典编纂者最想从语料库中获取的信息大多是语词的各种语言属性，包括形态、词类、句法模式、搭配成分、语义表征和使用语境等，因此需要对库内的各种用词进行标注。COCA 在 SWECCL 词类赋码的基础上设计了150多种标签，对全部4.5亿语料进行了逐条标注。譬如，连词类7个，如 CC(并列连词) 、CS(从属连词) 、CCB(转折并列连词)等; 形容词4个，如JJ(普通形容词)、JJR(形容词一般比较级)、JJT(形容词一般最高级)、JK(连接形容词); 限定词类13个，如 DA(前置或后置限定词) 、DA1( 后限定词单词形式) 、DAR(后限定词比较形式) 等; 名词类22个，NN(普通名词)、ND1(方向性单数名词)、NN1(单数普通名词)、NN2(复数普通名词) 、NNL1(单数方位名词) 等; NNO(数量词)、NNT1(单数时间名词)、NNU(度量单位词) 等; 动词31个，如 VB0(动词原形)、VDD(动词过去式)、VBM(系动词)、VBG(现在分词) 、VBN(过去分词)等; 代词19个，如PN(不定代词) 、PNQO(宾格 WH 代词) 、PNQS(主格 WH代词)和PNX1(反身不定代词) 等。其他还有副词、介词、冠词、程式(FO) 、未分类词( FU) 和外来词( FW) 等。 

除词类外，COCA 还对所有语料做了词汇语域和时间分布的标注，因为语词的意义和用法 与语域以及时间有着密切关系。语域维度分为口语、小说、流行杂志、报纸和学术期刊五大类型，语料按这五个类型基本呈均匀平衡分布; 时间维度分为: 1990—1994、1995—1999、 2000—2004、2005—2009、2010—2012 等五个时段，用户可以从这两个维度查询任何一个词的分布频率。为了语词的形态变体和同义词查询，库内还配置了词的屈折变化和同义词数据库。 

2 语料库的索引 

COCA 根据标注和用户的查询需要设计出一套索引句法( Search Syntax) 来满足“智能” 检索的需要。主要分以下几类: [pos](精确词类［vvg］) 、[pos* ]( 各种词类[v* ])、[lemma](原型词形态变体[speak])、[=word](同义词) 、word|word(两词比较)、*xx(以某前缀开头的词) 、x? xx( 含某字母的词) 、x? xx* ( 含某字母 + 某词缀的词) 、-word( 某词前成 分，若要限定词类则可加词类标签，如-[nn* ]) 等。利用上述句法成分和词类标签可以组合成各种复杂的索引句法结构，以便词典编纂者准确查找所需的各种信息。 

3 语料的智能化检索和显示 

这里说的“智能”并不是说它具有抽象思维和随机应变的能力，而是指检索系统通过识别语料库中的标注代码，按特定的索引句法提取语料数据。这样，用户便可以根据自己的查询需要，按一定句法规则来组织“检索模式”，语料库便可以比较准确地调出他们所需的各类信息。值得一提的是，即使是无限制的普通检索，COCA 也能做不同的结构和句法/搭配成分分类显示，如相同结构的例句放在一起，各种句法成分用不同颜色显示: 名词为蓝色、动词为紫色、形容词为绿色、副词为棕色、代词为灰色 、介词为黄色。这样，可以使用户对所显示内容一目了然。

参考：《语料库数据化发展趋势及词典学意义——兼谈美国当代英语语料库的数据化特征》
http://www.cnki.net/KCMS/detail/detail.aspx?QueryID=14&CurRec=25&recid=&filename=CSYA201505001&dbname=CJFDLAST2015&dbcode=CJFQ&pr=&urlid=&yx=&uid=WEEvREcwSlJHSldRa1FhcTdWYVFpK1NSeUY1MUgweVkrdjJxQ1ZqenJCQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!&v=MDA2MTBUTXFvOUZaWVI4ZVgxTHV4WVM3RGgxVDNxVHJXTTFGckNVUkx5ZVorWnBGeWptVjczSkpqN1NiN0c0SDk=


### 关于语料库开发的工具介绍:

1 索引工具

索引（Concordance）是呈现检索项和其共现语境的一种方式。检索项由词或短语以及词性赋码标记、通配符（wildcard）和正则表达式（regular expression）等构成。索引行可以按各种条件排序，常见的方法是按中心词左边或右边第N个词的字母顺序排序。研究者可对索引行数据做进一步分析，如搭配、类连接等，发现语言中反复出现的现象，从而揭示人们使用语言的模式（pattern）。

BFSU Collocator：搭配分析工具。搭配词是指在一定跨距内出现在节点词周围的词语，为了更好地研究节点词和搭配词间的意义关系，研究者可以利用各种统计算法计算两者间的搭配强度以排除搭配词中出现的高频功能词如“the”、“of”等。英语学习者可以通过搭配了解单词的典型用法，加深对词汇意义的理解。 BFSU Collocator提供MI、MI3、Z-Score、T-Score、Log-log和Log-likelihood等六种统计算法计算搭配强度，研究者可以根据具体研究问题选择合适的算法进行搭配研究。

BFSU Colligator：类连接分析工具。类连接是语法层面的搭配，包括语法类别之间以及词和语法类别之间的共现关系。研究者可以通过BFSU Colligator编写正则表达式，研究词类之间以及节点词与其周围词类间的关系。具体用法可参看许家金、熊文新（2009）。

BFSU CQPweb：基于网络的第四代语料库分析工具，能生成语料库词表、进行索引行分析、搭配计算和主题词分析。BFSU CQPweb采用了索引技术（indexing）提高了检索结果的响应时间，并可按照语料库中的各种元信息呈现检索结果。该平台目前已加载了多种语料库供用户免费使用，使研究者通过浏览器就可进行基于语料库的各种研究，降低了语料库语言学研究的技术门槛。BFSU CQPweb的网址为http://111.200.194.212/cqp/ ，具体用法可参看许家金、吴良平（待刊）。

BFSU CQPweb在线检索平台 网址http://111.200.194.212/cqp/

BFSU PatCount：BFSU PatCount用于统计文本中各种语言特征的出现频率，全面支持正则表达式及批量检索。研究者有时需要同时从文本中提取多种语言特征，比如Biber（1998）的多因素分析需要提取文本中的67种语言特征进行因子分析，如果逐一检索过于繁琐，而PatCount可以将67个检索式写入文本文件中，一次就可完成任务。输出结果以矩阵形式排列，每行为语言特征，每列为文本名称，便于后续进行因子分析。具体用法可参看梁茂成、熊文新（2008）。

ConcSampler & Concordance Randomizer ：在语料库研究中，研究者经常要面对成千上万条索引行，Sinclair（1999）建议每次随机抽取30条记录进行观察，总结其中的规律，然后再抽取30条记录，以此类推，直到无法观察到新的模式为止。BFSU开发的这两款工具可以帮助研究者或教师完成随机提取索引行的任务。
   
2 标注工具

标注是给语料库增添信息的过程，McEnery & Hardie（2012：29-31）认为可以把这些信息分为三类： 
  （1）元信息（Metadata）。如书面语中的文本类型（新闻、小说等）；口语中说活人的特征（性别、年龄等）。 
  （2）文本信息（Textual Mark-up）。如书面语中的段落、句子；口语中的停顿、重复等。 

（3）语言学信息（Linguistic Annotation）。如单词的词性、句子的结构和功能等。 
研究者根据上述三种标注类型可以从语料库中提取相应的信息开展研究，如女性和男性在口语中使用形容词的异同。

Metadata Encoder：元信息标注工具。可以在书面语语料库中添加文本作者、作者国籍和出版年代等信息；在口语语料库中添加说活人性别、年龄和社会地位等信息。用户可以在Metadata Encoder的配置文件中自行添加元信息的类别如<sex>、<age>等，然后在软件中针对每个类别进行标注，添加的信息会以下列格式保存在文本文件中：
   <sex>female</sex>
　　<age>22</age>
研究者可以根据文本中的元信息从语料库中提取某一类型的文本（如年龄在18-25之间的女性话语），建立子语料库（参看下面 Sub-corpus Creator的介绍）开展研究。

TreeTagger for Windows 2.0：TreeTagger是德国斯图加特大学Helmut Schmid开发的一款自动词性标注软件，采用宾州树库符码集（http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/Penn-Treebank-Tagset.pdf）。TreeTagger for Windows 2.0是其图形界面，支持英语、德语、法语、意大利语等四种语言的词性标注，同时还支持词形还原（lemmatization）功能。下面是该软件对句子“The cuisine of Xinjiang reflects the region's many ethnic groups and refers particularly to Uyghur cuisine.”的标注结果（用户可与下文BFSU Stanford POS Tagger的标注结果相比较发现两者符码的异同）：The_DT cuisine_NN of_IN Xinjiang_NP reflects_VVZ the_DT region_NN 's_POS many_JJ ethnic_JJ groups_NNS and_CC refers_VVZ particularly_RB to_TO Uyghur_NP cuisine_NN ._SENT
BFSU Stanford POS Tagger 1.0：Stanford POS Tagger是斯坦福大学自然语言处理小组开发的一款词性自动标注软件，采用宾州树库符码集（Marcus等：1993），符码准确率可达到96.97%（http://nlp.stanford.edu/software/pos-tagger-faq.shtml）。BFSU Stanford POS Tagger是其图形界面，它降低了原软件的操作难度，用户无需在命令行中输入命令和参数就可对文本进行符码。运行该软件前需安装JAVA虚拟机（http://www.java.com/en/download/）。下面是该软件对句子“The cuisine of Xinjiang reflects the region's many ethnic groups and refers particularly to Uyghur cuisine.”的标注结果（用户可与上文TreeTagger for Windows的标注结果相比较发现两者符码的异同）：The_DT cuisine_NN of_IN Xinjiang_NNP reflects_VBZ the_DT region_NN 's_POS many_JJ ethnic_JJ groups_NNS and_CC refers_VBZ particularly_RB to_TO Uyghur_NNP cuisine_NN ._.

BFSU Stanford Parser 1.0：Stanford Parser是斯坦福大学自然语言处理小组开发的一款句法自动切分软件，可以切分句子的短语结构（Phrase Structure）和依存关系（Dependency Relation）。BFSU Stanford Parser 1.0是其图形界面，它降低了原软件的操作难度，用户无需在命令行中输入命令和参数就可对文本进行句法切分。利用该软件对句子“The cuisine of Xinjiang reflects the region's many ethnic groups and refers particularly to Uyghur cuisine.”进行句法切分后可以输出以下两种格式：
   (1）括号格式的短语结构。其中NP、VP和PP分别代表名称短语、动词短语和介词短语
   (2）依存关系。其中det表示限定词与名词之间的关系；nsubj表示句子主语与谓语间的关系。其它依存关系可参考de Marneffe & Manning（2008）

BFSU Qualitative Coder 1.0 & BFSU Qualitative Explorer：在语法、语义和语用等层面的许多语言现象无法通过软件自动标注，如对话中言语行为（dialogue act）的种类（请求、疑问赞同等）、小说中人物话语和思想的呈现方式（直接应用、间接应用等）以及与某一语言特征如情态动词共现的其它语言特征，如主语的生命度（有灵、无灵主语），后续实义动词的语义种类（状态、过程动词等）、所在句子的类型（陈述句、疑问句等），都需要手工进行标注。研究者可以利用
BFSU Qualitative Coder自行设定语言现象的种类，在文本中进行手工标注。标注完成后可以用BFSU Qualitative Explorer统计各种语言现象出现的频率。
   
3 文本处理工具
BFSU Sentence Segmenter 1.0：我们通常以逗号、句号、问号和感叹号等标点符号作为英文句子间的分界点，但如果采取上述方法进行自动分句的话会将一些缩略词的词尾（如Dr.、Mrs.、Ph.等）误判为句子结尾，造成切分错误。BFSU Sentence Segmenter支持用户自定义缩略词表，可以通过扩充缩略词表提高句子切分的准确率。

Sub-corpus Creator：该软件利用语料库中的文本名称以及文本中的元信息建立研究者所需的子语料库。如CROWN语料库中以A、B、C开头的文件代表新闻语体，研究者可以用(A|B|C)\S+\.txt这一正则表达式选定这类文本。同时该软件还可以选择文本内容，如研究介绍中国的新闻报道可以在文本内容中输入表达式china|chinese。
##
Readability Analyzer 1.0：通过对文本平均词长、平均句长、形符/类符比等特征的统计计算英文文本的易读性。
   
4 数据驱动学习工具

数据驱动学习（Data-driven Learning）是指老师和学生通过检索语料库、分析索引行的方法教授/学习二语/外语的过程。这一方法强调学生的主动性，鼓励他们通过“发现式”的学习方法归纳语言在真实语境下的使用规律。

BFSU Sentence Collector 1.0：用于英语教学的索引工具，内置大学英语教材语料库（http://www.corpus4u.org/forum/showthread.php?t=3217）与四级词表。与上文介绍的索引工具不同的是其呈现方式为含有检索词的整句；另外用户可以根据句子长度和句子中的新词数（未出现在四级词表中的单词）来筛选例句。该工具支持正则表达式检索，如输入as \S+ as可以检索出含有as well as、as much as等短语的例句。

BFSU NewWord Marker 1.0：新词标记工具。根据用户指定的基准词表（如四、六级词表）输出特定文本中含有新词的句子，每句后列出未出现于词表中的新词以及该句所在的文件名称。进行新词标记前用户可以用BFSU Sentence Segmenter 1.0先对文本进行分句


###大数据语音语料库的社会标注

近年来，随着互联网和移动终端的普及，各种形式的大数据如洪水般涌来，为大数据语音语料库的过滤、标注和运用带来了挑战。而在技术研究上，将深度学习成功引入到声学模型训练中，使得语音识别研究获得重大突破］。相比传统 Gauss混合模型（ Gaussian mixture model，GMM）， 基于深度学习的深度神经网络（deep neutral network, DNN）模型显示出了对大数据训练语料的更强烈的需求。本文作者在实验中发现，每次增加训练语料规模，都会带来DNN 识别性能的明显提升，其性能的极限还远没有达到，而且语料数据本身的质量和丰富程度对性能影响也非常明显， 这说明大数据语料的积累和精细标注将会对DNN的更深入探索具有重要意义。
由于数据量巨大，在实际应用中不得不考虑标注的成本、效率和质量，这3个要素缺一不可。社会标注是指由大众用户产生的、对资源内容进行自下而上标注分类的体系，是利用大众智慧来对资源进行配置和挖掘的一种手段。社会标注是 Web 2.0基础上产生的，其关键技术在于使大众可以开放自由地分享内容, 而社会化的标签已经成为互联网上的一种重要的信息组织方式。
    
最典型的系统包括美国宾夕法尼亚大学早在2005年就开发出来的PennTags系统，以及美国密歇根大学在2008年推出的MTagger系统。社会标注的快速发展则是以提供标注服务的网站的流行为标志的，这些网站允许用户使用简短的字词对自己喜爱和关注的网页进行标注并保存在自己的账号中，极大方便了用户对个人网络资源的组织和管理。比较有名的网站有：网络书签系统Delicious，图片标注分享系统Flickr，对学术论文进行标注的文献检索系统 CiteULike ，对MP3音乐进行标注和偏好推荐的系统LastFM。而一些博客和论坛等，也或多或少提供了类似主题词的标签功能。

标注模型
    
社会标签是一种元数据， 由〈User，Resource，Tag〉三元组组成，并通过分析用户、资源和标签之间的关系进行标签理论和标签推荐的研究。然而，此类研究一般是仅分析对象间的关系，没有考虑资源本身的特征，标签的内涵也仅限于主题词。因此，针对语音数据的标注，将语音资源的特征考虑在内，并将标签对象拓展为更广义的标记，一种标记是与语音内容直接相关的词语串，另一种标记是与语音内容间接相关并具有一定标引功能的标签。
标注模型包含３类对象：参与标注的用户、 标记本身以及被标记的数据。本文的社会标注系统是一个六元组A={用户，标记，数据，节点之间表示标注关系的超编集，不包含重复词汇的词表，语音}。
    
在标注系统中，每个用户的标注是用大量标记体现出来的， 而每个数据将会获得多个用户的标记，每个标记都是用户针对数据的一种转录和理解， 这些标记体现了大众智慧。

该标注系统可以分成数据流控制和预处理模块、社会标注模块、质量自动控制模块、人工审核模块、 标注信息存储模块、标注信息运用模块６个部分。其中， 数据流从口语翻译系统源源不断流进， 数据均为终端用户的实际测试语音。由于用户和数据的响应并发量巨大，而标注系统的运行必须要与数据流相匹配。因此在数据送入标注模块之前增加数据流缓存环节，并根据标注进度来进行有效调控。同时，由于原始数据流中混有大量无效的语音样本，这些语音是没有必要进行标注的，包括太短的语音、没有发声的语音、发音内容杂乱无章的语音等， 通过信号层检测和ASR识别结果的置信度进行过滤。

标注任务的组织和分解

社会标注的难点在于如何保证系统能够高效运行，如何让大众用户持续为系统作出贡献。“兴趣＋收获 ＋ 报酬”的三位一体的社会标注方案，通过对标注任务的有效组织和分解来增强趣味性，通过将标注任务与语言学习进行结合来提高用户的收获感和成就感，通过对用户标注数量和质量的积分累计来进行报酬兑换， 以便进一步提高用户标注的积极性。

标注质量控制机制

社会标注在效率和成本方面具有优势， 而质量控制则是其短板。因此，首先必须制定相应的标注规范，例如词汇的单复数、连接符号、标记符号、口语现象处理、异常处理等，这些规范会在用户注册时进行展示，用户同意条款才能开始标注。而在标注过程中，后台会记录用户违反规范的次数，并适时给用户以警告，如果多次警告无效，则会冻结用户的标注权限。采用“3层检验” 机制，包括多用户标注一致性检验、与ASR结果一致性检验和专家审核检验3个层面。

参考文献：《大数据语音语料库的社会标注技术》



### 基于语料库的机器翻译

1989年以来，机器翻译的发展进入了一个新纪元。这个新纪元的重要标志是，在基于规则的技术中引入了语料库方法，其中包括统计方法，基于实例的方法，通过语料加工手段使语料库转化为语言知识库的方法，等等。近年来，基于语料库的机器翻译系统发展很快，取得了突出的成绩。

基于语料库的机器翻译系统可分为两种：基于统计的机器翻译系统和基于实例的机器翻译系统。这两种都使用语料库作为翻译知识的来源，所以可以统称为基于语料库的机器翻译方法。

这两种方法的区别如下：

* 在基于统计的机器翻译方法中，知识的表示是统计数据，而不是语料库本身；翻译知识的获取是在翻译之前完成，翻译的过程中不再使用语料库。在方法上更强调从数学上建立统计模型。
* 在基于实例的机器翻译方法中，双语语料库本身就是翻译知识的一种表现形式 （不一定是唯一的），翻译知识的获取在翻译之前没有全部完成，在翻译的过程中还要查询并利用语料库。在方法上主要是从机器学习的角度通过翻译实例进行推理。

参考：[基于语料库的机器翻译系统](http://www.cnki.net/KCMS/detail/detail.aspx?QueryID=0&CurRec=17&recid=&filename=SYBX201001010&dbname=CJFD2010&dbcode=CJFQ&pr=&urlid=&yx=&uid=WEEvREcwSlJHSldRa1FhcEE0L01SbTN0dHZidG1mUHQ5a2xEK3BBOEd1az0)


###Google's Neural Machine Translation System
论文：https://arxiv.org/pdf/1609.08144.pdf

神经机器翻译是一种端到端的机器翻译方法，并且有超越基于短语的机器翻译方法的潜力。不幸的是，在之前由于深度的循环神经网络在训练和预测阶段计算量都非常大，导致很难应用在庞大的数据集上，模型也不敢搞得太大，因此效果并不是很理想。这个机器翻译模型是一个有8个encoder和8个decoder组成的深度LSTM，为了加速在预测阶段的速度，在预测时采用了低精度运算。注意力机制加在ecoder的顶部和decoder的底部，并且encoder和decoder的不同阶段的运算使用了多GPU并行，提高了计算速度。
模型在几个数据集上进行了评估，其中最大的数据集WMT ENtoFr数据集包括36M各句子对。

